import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import pandas as pd
import cv2
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from pytube import YouTube

# Define a custom CNN neural network architecture
class CustomCNN(nn.Module):
    def __init__(self):
        super(CustomCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(128 * 6 * 6, 256)
        self.relu4 = nn.ReLU()
        self.fc2 = nn.Linear(256, 7)  # 7 classes for emotions

    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.pool3(self.relu3(self.conv3(x)))
        x = self.flatten(x)
        x = self.relu4(self.fc1(x))
        x = self.fc2(x)
        return x

# Custom dataset class for FER2013
class FER2013Dataset(Dataset):
    emotions = {
        0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy',
        4: 'Sad', 5: 'Surprise', 6: 'Neutral'
    }

    def __init__(self, csv_file, transform=None):
        self.data = pd.read_csv(csv_file)
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pixels = self.data.iloc[idx, 1].split(' ')
        pixels = [int(pixel) for pixel in pixels]
        image = torch.tensor(pixels, dtype=torch.float32).view(48, 48) / 255.0

        emotion_label = int(self.data.iloc[idx, 0])

        # Convert tensor to PIL Image
        image = Image.fromarray((image.numpy() * 255).astype('uint8'))

        if self.transform:
            image = self.transform(image)

        return image, emotion_label

# Load FER2013 dataset
fer2013_dataset = FER2013Dataset(csv_file='Book2.csv', transform=transforms.ToTensor())

# Split dataset into training and testing sets
train_size = int(0.8 * len(fer2013_dataset))
test_size = len(fer2013_dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(fer2013_dataset, [train_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Initialize the custom CNN model, loss function, and optimizer
custom_cnn_model = CustomCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(custom_cnn_model.parameters(), lr=0.001)

# Training loop with tracking training accuracy history
num_epochs = 20  # Increased the number of epochs
train_accuracy_history = []

for epoch in range(num_epochs):
    custom_cnn_model.train()
    total_correct = 0
    total_samples = 0
    
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = custom_cnn_model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        _, predictions = torch.max(outputs, 1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate training accuracy for the current epoch
    accuracy = total_correct / total_samples
    train_accuracy_history.append(accuracy)
    print(f'Epoch {epoch + 1}/{num_epochs}, Training Accuracy: {accuracy:.4f}')

# Plot the training accuracy history
plt.plot(train_accuracy_history, label='Training Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Save the trained custom CNN model
torch.save(custom_cnn_model.state_dict(), 'custom_cnn_emotion_model.pth')

# Real-time camera input (use the custom CNN model for inference)
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()

    # Preprocess the frame
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    frame_resized = cv2.resize(frame_gray, (48, 48))
    frame_normalized = frame_resized / 255.0
    frame_tensor = torch.tensor(frame_normalized, dtype=torch.float32).view(1, 1, 48, 48)

    # Make a prediction using the trained custom CNN model
    custom_cnn_model.eval()
    with torch.no_grad():
        output = custom_cnn_model(frame_tensor)
        _, prediction = torch.max(output, 1)
        emotion_label = prediction.item()

    # Display the result on the frame with a bounding box
    emotion_text = FER2013Dataset.emotions[emotion_label]
    cv2.putText(frame, f'Emotion: {emotion_text}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    
    # Draw bounding box
    cv2.rectangle(frame, (10, 10), (frame.shape[1] - 10, frame.shape[0] - 10), (255, 0, 0), 2)

    # Display the frame
    cv2.imshow('Facial Emotion Recognition', frame)

    # Break the loop if 'q' key is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the camera and close the window
cap.release()
cv2.destroyAllWindows()
